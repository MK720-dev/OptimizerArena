
# OptimizerArena

OptimizerArena is a research-grade visualization platform for analyzing neural network optimization algorithms through interactive 3D loss-landscape exploration. This document provides a comprehensive technical overview of the systemâ€™s mathematical foundationsâ€”particularly the role of Principal Component Analysis (PCA) in enabling high-dimensional weight-trajectory visualizationâ€”along with backend algorithmic details, frontend architecture, and design decisions that balance mathematical correctness with visual interpretability.

---

## Overview

OptimizerArena extends a custom neural-network training engine with advanced tools for:

* Comparing optimizers (SGD, RMSProp, Adam, BFGS, etc.)
* Capturing weight trajectories during training
* Performing PCA on high-dimensional weight vectors
* Sampling 2D slices of the loss landscape
* Rendering high-fidelity 3D visualizations of optimization paths vs. local geometry

The platform is designed for research, teaching, and the empirical study of optimization behavior in deep learning.

## Visual Demo

Quick demo available here:  
ğŸ‘‰ [Download / Watch Demo 1](./OptimizerArenaDemo1.mp4)
ğŸ‘‰ [Download / Watch Demo 2](./OptimizerArenaDemo2.mp4)

---
# 1. Web App Architecture Overview

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       Python Backend            â”‚
                â”‚        (FastAPI API)            â”‚
                â”‚   - Training engine             â”‚
                â”‚   - Optimizers (SGD, BFGS,...)  â”‚
                â”‚   - PCA + Loss Surface          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚  HTTP (JSON)
                                â”‚  /train, /pca
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    Vite Dev Server         â”‚
                 â”‚  (Node.js Proxy Gateway)   â”‚
                 â”‚                            â”‚
                 â”‚  Proxies: /api/* â†’ FastAPI â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚  Local WebSocket + HTTP
                                 â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       React Frontend      â”‚
                 â”‚  - Controls Panel UI      â”‚
                 â”‚  - Arena3D Visualization  â”‚
                 â”‚  - Plotly Loss Surface    â”‚
                 â”‚  - Optimizer Trajectory   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## FastAPI (Python backend)
Implements all core computation: neural-network training, optimizers, PCA, loss-surface reconstruction.
Exposes routes like /api/train and /api/pca.

## Node.js / Vite dev server
Acts as a reverse proxy, forwarding all /api/* requests to FastAPI during development.
Also serves the bundled React assets.

## React Frontend (Vite client)
Renders UI, collects user parameters, triggers backend training calls, and visualizes:

- optimizer trajectories

- PCA projections

- 3D loss surfaces with Plotly

# 2. Backend Architecture & Training Pipeline

## 2.1 Correct Loss Function Selection

Previous versions incorrectly applied MSE to all tasks, including binary classification. This caused issues such as vanishing gradients and incorrect probabilistic assumptions.

OptimizerArena now **automatically selects the correct loss function**:

| Task Type                 | Loss                  | Activation | Rationale                                                  |
| ------------------------- | --------------------- | ---------- | ---------------------------------------------------------- |
| Regression                | MSE                   | Linear     | Natural for continuous outputs                             |
| Binary Classification     | Binary Cross-Entropy  | Sigmoid    | Proper probabilistic interpretation                        |
| Multiclass Classification | Softmax Cross-Entropy | Softmax    | Stable and mathematically aligned with class probabilities |

Additional backend improvements:

* Stable softmax + cross-entropy derivative formulation
* Automatic loss selection in `NeuralNetwork.set_loss()`
* Gradient clipping for numerical stability

---

## 2.2 BFGS Optimizer Redesign

The earlier implementation incorrectly nested BFGS iterations inside the epoch loop.

### After Fix:

* BFGS performs **one full quasi-Newton update per epoch**
* `max_iter` applies **only to line search convergence**
* Wolfe conditions enforced correctly
* Weight snapshots captured consistently at epoch granularity
* Trajectories correctly reflect the actual optimization path

This yields smoother, more mathematically valid BFGS behavior.

---

## 2.3 Weight Capture Strategy

Weights are stored:

* At **epoch 0**
* At **every epoch afterward**

This ensures:

* High-quality PCA input
* Matching array lengths across losses, weights, projections
* Clear visualization of trajectory evolution through training

---

## 2.4 Global Minâ€“Max Normalization

Before:
Surface, trajectory, and reconstructed trajectory were normalized **independently**, causing scale mismatch and floating trajectories.

Now:

```python
scaled = (value - surface_min) / (surface_max - surface_min)
```

All elements use **one global normalization**, extracted from the loss surface only.

Benefits:

* A unified and mathematically consistent scale
* Accurate trajectory-to-surface alignment
* No misleading distortions

---

# 3. PCA-Enhanced Visualization

Neural networks operate in extremely high-dimensional parameter spaces. PCA provides a principled way to reduce this to 2D or 3D for visualization.

---

## 3.1 The Visualization Challenge

We want to:

* Visualize the shape of the loss landscape
* Visualize optimizer trajectories
* Compare optimizers geometrically

But:

* Weight space is typically thousands to millions of dimensions
* Loss is a function $( L : \mathbb{R}^n \to \mathbb{R} )$
* We cannot sample or visualize beyond 3 dimensions

Thus PCA becomes essential.

---

## 3.2 PCA as the Solution

PCA extracts the dominant directions of variation in the optimizer trajectory.

It provides:

1. A meaningful 2D/3D coordinate system
2. A common frame for comparing different optimizers
3. A 2D plane (PC1â€“PC2) from which to sample the loss surface
4. Variance-explained metrics to assess projection quality

---

## 3.3 Mathematical Summary

### Step 1 â€” Collect weight snapshots

$$[
w_0, w_1, \dots, w_T \in \mathbb{R}^n
]$$

### Step 2 â€” Form the data matrix

$$
M \in \mathbb{R}^{(T+1)\times n}
$$

### Step 3 â€” Center the data

$$
X = M - \mu
$$

### Step 4 â€” Apply SVD

$$
X = U S V^T
$$

Right-singular vectors ( V ) are the **principal component directions**.

### Step 5 â€” Select first ( k ) PCs

Typically ( k = 2 ) or ( k = 3).

### Step 6 â€” Project weights into PCA space

$$
z = (w - \mu)V_k
$$

### Step 7 â€” Explained variance

$$
\text{var}_i = \frac{s_i^2}{\sum_j s_j^2}
$$

---

# 4. Weight Reconstruction & Loss Surface Rendering

## 4.1 Forward Projection

$$
z = (w - \mu)V_k
$$

## 4.2 Inverse Reconstruction

$$
\hat w = \mu + \sum_{i=1}^k z_i v_i
$$

This reconstructs weights for computing loss values at grid points or trajectory points.

---

## 4.3 Reconstruction Strategies

### **A. Trajectory Reconstruction â€” Use All PCs (k = larger)**

* Accurate loss evaluation
* Low reconstruction error
* Faithful reproduction of true training behavior

### **B. Loss Surface â€” Use Only PC1 & PC2**

We define:

$$
w(\alpha,\beta) = \mu + \alpha v_1 + \beta v_2
$$

Sampling this plane yields a **dense 2D grid** for the 3D surface plot.

Why only two?

* 3D visualization requires 2D domain + 1D loss
* Higher dimensions cannot be visualized
* PC1 & PC2 capture the dominant variation

---

## 4.4 Why the Trajectory Appears on the Surface

Even though trajectories use more PCs:

* PC1 & PC2 capture most meaningful motion
* Higher PCs often correspond to flat or low-impact directions
* Loss is effectively a function of the first two PCs
* Shared normalization ensures alignment

Small deviations arise from reconstruction error, but are usually imperceptible.

---

# 5. Frontend Architecture

The frontend (React + Three.js) supports:

* Real-time 3D rotation, zoom, and pan
* Animated optimizer paths
* Loss-surface interpolation and color mapping
* Responsive UI with control menus

Communication with the backend occurs via REST endpoints for:

* PCA projections
* Trajectory data
* Reconstructed weights
* Loss surface grids

---

# 6. Datasets overview

OptimizerArena supports multiple datasets spanning regression, binary classification, and multiclass classification, allowing users to evaluate optimizer behavior across a diverse set of learning problems. All datasets are standardized into the format:

```
X : (n_features, n_samples)
y : (1 or C, n_samples)
meta : { input_dim, output_dim, task_type, name }
```

## Synthetic Regression Datasets

Synthetic regression datasets are procedurally generated to provide controlled difficulty levels.
All synthetic inputs follow:

- Input sampling:

	$$ğ‘‹ âˆˆ ğ‘…^2 , ğ‘‹ âˆ¼ ğ‘(0,1)$$

- Target generation:

	$$ğ‘¦ = ğ‘“(ğ‘‹) + ğœ€, ğœ€âˆ¼ğ‘(0,0.1)$$

Depending on the selected function func_variant âˆˆ { "simple", "medium", "complex" }, the target values come from one of the following analytic functions:

### Simple Function (Linear)

$$ğ‘¦ = 3ğ‘¥_1 + 2ğ‘¥_2 + ğœ€$$

- Low curvature

- Good for verifying correctness of optimizers

- Produces an almost-convex loss surface

### Medium Function (Mildly Nonlinear)
$$ğ‘¦ = ğ‘¥_1^2 + sin(ğ‘¥_2) + ğœ€$$

- Introduces moderate nonlinearity

- Contains local curvature variations

- Useful for evaluating adaptive optimizers (Adam, RMSProp)

### Complex Function (Highly Nonlinear)
$$ğ‘¦ = sin(ğ‘¥_1ğ‘¥_2) + 0.5ğ‘¥_1^3 âˆ’ ğ‘¥_2^2 + ğœ€$$

- Strong nonlinearity and multimodality

- Produces a rugged loss landscape

- Ideal for contrasting first-order vs. second-order behavior

### Usage in OptimizerArena

Synthetic datasets are configured by:

- dataset_name="synthetic"

- func_variant âˆˆ {"simple", "medium", "complex"}
## California Housing (Real Regression Dataset)

âœ” Task: Regression
âœ” Source: Scikit-learnâ€™s fetch_california_housing()
âœ” Shape:

Features: 8 (median income, house age, rooms, population, etc.)

Samples: ~20,000

This large real-world dataset allows testing:

- Optimizer scalability

- Sensitivity to feature scaling
 
- Behavior on noisy, heterogeneous data

## Breast Cancer (Binary Classification)

âœ” Task: Binary classification
âœ” Source: load_breast_cancer()
âœ” Labels: 0 (benign), 1 (malignant)
âœ” Features: 30

Used to benchmark:

- Binary cross-entropy vs MSE

- Stability of optimizers on real classification tasks

- Decision boundary sharpness for small networks

## Iris Dataset (Multiclass Classification)

âœ” Task: Multiclass (3 classes)
âœ” Source: load_iris()
âœ” Features: 4
âœ” Classes: {0, 1, 2}

Used for:

- Testing softmax cross-entropy

- Visualizing multiclass loss geometry

- Understanding optimizer behavior on well-conditioned low-dimensional data

Summary Table
Dataset Name	Task Type	Complexity	Notes
Synthetic (Simple)	Regression	Low	Linear model test
Synthetic (Medium)	Regression	Medium	Mild nonlinearities
Synthetic (Complex)	Regression	High	Rugged loss landscape
California Housing	Regression (real)	Medium	Large real dataset
Breast Cancer	Binary Classification	Lowâ€“Med	BCE loss evaluation
Iris	Multiclass Classification	Low	Softmax model test

---

# 7. Quick Start

### **Backend (Terminal 1)**

```bash
cd backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
```

### **Frontend (Terminal 2)**

```bash
cd frontend
npm install
npm run dev
```

Open the app at:

```
http://localhost:5173
```

---

# 8. Repository Structure

```
OptimizerArena/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ venv/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ backend.pyproj
â”‚   â”œâ”€â”€ backend.pyproj.user
â”‚   â”œâ”€â”€ neural_network.py
â”‚   â”œâ”€â”€ optimizers.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ .vscode/
â”‚   â”œâ”€â”€ node_modules/
â”‚   â”œâ”€â”€ obj/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ (React components, hooks, styles, utils, etc.)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ CHANGELOG.md
â”‚   â”œâ”€â”€ eslint.config.js
â”‚   â”œâ”€â”€ frontend.esproj
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tsconfig.app.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tsconfig.node.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

```

---

# 9. Future Work

This project opens the door to several meaningful extensions that can greatly expand the analytical depth and usability of OptimizerArena:

## More Advanced Loss-Landscape Modeling
Incorporate richer surrogate models (Gaussian Processes, kernel regression, neural fields) to generate smoother, more accurate 3D surfaces and improve alignment between reconstructed trajectories and the true loss geometry.

## Support for Additional Optimizers
Expand beyond SGD/Adam/RMSProp/BFGS to include Momentum SGD, Nesterov, AdamW, L-BFGS, K-FAC, Shampoo, and other modern second-order optimizers to enable broader comparative studies.

## Trajectory Animation & Multi-Optimizer Visualization
Add animated optimizer avatars, playback controls, and side-by-side trajectories for comparing how different optimizers traverse the same loss landscape.

## Expanded Dataset Library + User Uploads
Support more real-world datasets, richer synthetic functions, and user-uploaded datasetsâ€”making the platform robust across regression, binary classification, and multi-class settings.

## Refined PCA & Dimensionality Reduction Tools
Add per-fold PCA, user-selectable number of components, and reconstruction-error diagnostics. Investigate nonlinear alternatives like UMAP, t-SNE, or Isomap for complex trajectory manifolds.

## Unified Diagnostics Dashboard
Integrate loss curves, gradient norms, learning-rate schedules, curvature estimates (e.g., Hessian approximations), and per-epoch statistics into a cohesive analytics panel.

## Improved UI/UX and Visualization Controls
Enhance the interface with a cleaner control panel, dark mode, 2D/3D toggles, export options (PNG/JSON), and a more polished layout for professional use.

## Modular Plugin-Style Architecture
Allow researchers to plug in custom optimizers, datasets, activation functions, or visualization modulesâ€”turning OptimizerArena into an extensible research framework.

## Stability, Sensitivity, and Meta-Learning Studies
Use recorded trajectories to study optimizer robustness to initialization, noise, and hyperparameters, and explore meta-learning strategies to automate optimizer selection.

## Higher-Dimensional Landscape Exploration
Explore slicing techniques beyond 2-PC projection: use multi-plane slicing, 3-PC volumetric views, and neighborhood curvature estimation to capture more complex geometry.

---

# 10. References
## Optimization & Neural Network Training

- Nocedal, J., & Wright, S. (2006). Numerical Optimization (2nd ed.). Springer.
â€” Standard reference for quasi-Newton methods (BFGS, line search, curvature conditions).

- Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR.
â€” Introduces Adam, the adaptive optimizer used as a baseline.

## Loss Landscape Visualization & PCA Geometry

- Li, H., Xu, Z., Taylor, G., & Goldstein, T. (2018). Visualizing the Loss Landscape of Neural Nets. NeurIPS.
â€” Foundation for PCA-based projection of high-dimensional weight trajectories.

- Jolliffe, I. T. (2002). Principal Component Analysis. Springer.
â€” Classical treatment of PCA, eigen-decomposition, and dimensionality reduction.

## Surrogate Modeling & Surface Reconstruction

- Forrester, A., Sobester, A., & Keane, A. (2008). Engineering Design via Surrogate Modelling. Wiley.
â€” Background for surrogate-based surface approximation in high-dimensional spaces.

## Neural Optimization Dynamics

- Bottou, L., Curtis, F. E., & Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. SIAM Review.
â€” Analysis of SGD regimes, convergence, and curvature behavior.

## Additional Useful Background

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
â€” Chapters on optimization, curvature, and training instability.

# 11. Citation

If you use OptimizerArena for research or teaching, please cite this repository.

---
















